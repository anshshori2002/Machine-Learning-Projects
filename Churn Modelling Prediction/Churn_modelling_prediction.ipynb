{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Modelling Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement :- \n",
    "A bank uses these independent variables and analyse the behaviour of customer to see wether they leave the bank or stay. Now the bank has to create a predictive model based on this data set in order to predict the behaviour of new customers. This predictive model has to predict for any new customer wether he or she will stay or leave the bank so that the bank can offer something special for the customer, whom the predictive model predicts that they will leave the bank. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Dataset :-\n",
    "Independent Variables are:-\n",
    "1. Customer id\n",
    "2. Surname \n",
    "3. Credit score\n",
    "4. Geography \n",
    "5. Gender \n",
    "6. Age\n",
    "7. Tenure \n",
    "8. Balance\n",
    "9. NumofProducts\n",
    "10. HasCrCard\n",
    "11. IsActiveMember\n",
    "\n",
    "Dependent Variables are:-\n",
    "1. Exited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
       "0             1    15634602   Hargrave          619    France  Female   42   \n",
       "1             2    15647311       Hill          608     Spain  Female   41   \n",
       "2             3    15619304       Onio          502    France  Female   42   \n",
       "3             4    15701354       Boni          699    France  Female   39   \n",
       "4             5    15737888   Mitchell          850     Spain  Female   43   \n",
       "...         ...         ...        ...          ...       ...     ...  ...   \n",
       "9995       9996    15606229   Obijiaku          771    France    Male   39   \n",
       "9996       9997    15569892  Johnstone          516    France    Male   35   \n",
       "9997       9998    15584532        Liu          709    France  Female   36   \n",
       "9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n",
       "9999      10000    15628319     Walker          792    France  Female   28   \n",
       "\n",
       "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0          2       0.00              1          1               1   \n",
       "1          1   83807.86              1          0               1   \n",
       "2          8  159660.80              3          1               0   \n",
       "3          1       0.00              2          0               0   \n",
       "4          2  125510.82              1          1               1   \n",
       "...      ...        ...            ...        ...             ...   \n",
       "9995       5       0.00              2          1               0   \n",
       "9996      10   57369.61              1          1               1   \n",
       "9997       7       0.00              1          0               1   \n",
       "9998       3   75075.31              2          1               0   \n",
       "9999       4  130142.79              1          1               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "0           101348.88       1  \n",
       "1           112542.58       0  \n",
       "2           113931.57       1  \n",
       "3            93826.63       0  \n",
       "4            79084.10       0  \n",
       "...               ...     ...  \n",
       "9995         96270.64       0  \n",
       "9996        101699.77       0  \n",
       "9997         42085.58       1  \n",
       "9998         92888.52       1  \n",
       "9999         38190.78       0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into Dependent and Independent values \n",
    "X=dataset.iloc[:,3:13].values\n",
    "Y=dataset.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label and One-Hot Encoding\n",
    "Converting values into numerial values (For example in above dataset where column Geography and Gender ) in order to work our algorithm properly it is known as Encoding.\n",
    "For an instance in the above dataset there is a column 'gender' in which we can assign 0 to Female and 1 to male this is known as Label encoding. \n",
    "\n",
    "In the another instance there is a column 'geography' where we use hot encoding instead of label encoding. In the hot encoding each label is identified ob array of integers rather than array of integer.\n",
    "\n",
    "With the help of sklearn library we can encode the values by ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_X_2 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,2]=labelencoder_X_2.fit_transform(X[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 0 ... 1 1 101348.88]\n",
      " [608 'Spain' 0 ... 0 1 112542.58]\n",
      " [502 'France' 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 0 ... 0 1 42085.58]\n",
      " [772 'Germany' 1 ... 1 0 92888.52]\n",
      " [792 'France' 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this one hot encoding is used to convert the values of Geography into array of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=ColumnTransformer([('ohe',OneHotEncoder(),[1])],remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(ct.fit_transform(X),dtype=np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0.0' '0.0' '619' ... '1' '1' '101348.88']\n",
      " ['0.0' '1.0' '608' ... '0' '1' '112542.58']\n",
      " ['0.0' '0.0' '502' ... '1' '0' '113931.57']\n",
      " ...\n",
      " ['0.0' '0.0' '709' ... '0' '1' '42085.58']\n",
      " ['1.0' '0.0' '772' ... '1' '0' '92888.52']\n",
      " ['0.0' '0.0' '792' ... '1' '0' '38190.78']]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into Training and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Feature Scaling :-\n",
    "Feature scaling is a technique used to standardize the independent features present in the data in a fixed range. As we can see in the data, set ranges of attributes are not fixed to a specific range. Those huge range require a lot of time for calculation. So, to overcome this problem, we perform feature scaling. One thing you need t be aware of is that it's better to always perform feature scaling in deep learning no matter what range of value you're in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might noticed in below line that we use fit_trandform in training set, and we only use fit_transform in testing set. We do so because we learn the parameter of scaling on the training data and at the same time, we scale the trainig data. Now for the test data, we use the scaling parameters learned on the training data to scale the teat data, so there is no need for the fit_transform for the test data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5698444   1.74309049  0.16958176 ...  0.64259497 -1.03227043\n",
      "   1.10643166]\n",
      " [ 1.75486502 -0.57369368 -2.30455945 ...  0.64259497  0.9687384\n",
      "  -0.74866447]\n",
      " [-0.5698444  -0.57369368 -1.19119591 ...  0.64259497 -1.03227043\n",
      "   1.48533467]\n",
      " ...\n",
      " [-0.5698444  -0.57369368  0.9015152  ...  0.64259497 -1.03227043\n",
      "   1.41231994]\n",
      " [-0.5698444   1.74309049 -0.62420521 ...  0.64259497  0.9687384\n",
      "   0.84432121]\n",
      " [ 1.75486502 -0.57369368 -0.28401079 ...  0.64259497 -1.03227043\n",
      "   0.32472465]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the output of the below line, we can see all values have been scaled to a fesiable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>1.743090</td>\n",
       "      <td>0.169582</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>-0.464608</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>-1.215717</td>\n",
       "      <td>0.809503</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>-1.032270</td>\n",
       "      <td>1.106432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.754865</td>\n",
       "      <td>-0.573694</td>\n",
       "      <td>-2.304559</td>\n",
       "      <td>0.916013</td>\n",
       "      <td>0.301026</td>\n",
       "      <td>-1.377440</td>\n",
       "      <td>-0.006312</td>\n",
       "      <td>-0.921591</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.968738</td>\n",
       "      <td>-0.748664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>-0.573694</td>\n",
       "      <td>-1.191196</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>-0.943129</td>\n",
       "      <td>-1.031415</td>\n",
       "      <td>0.579935</td>\n",
       "      <td>-0.921591</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>-1.032270</td>\n",
       "      <td>1.485335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>1.743090</td>\n",
       "      <td>0.035566</td>\n",
       "      <td>0.916013</td>\n",
       "      <td>0.109617</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.473128</td>\n",
       "      <td>-0.921591</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>-1.032270</td>\n",
       "      <td>1.276528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>1.743090</td>\n",
       "      <td>2.056114</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>1.736588</td>\n",
       "      <td>1.044737</td>\n",
       "      <td>0.810193</td>\n",
       "      <td>0.809503</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.968738</td>\n",
       "      <td>0.558378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>1.754865</td>\n",
       "      <td>-0.573694</td>\n",
       "      <td>-0.582970</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>-0.656016</td>\n",
       "      <td>-0.339364</td>\n",
       "      <td>0.703104</td>\n",
       "      <td>0.809503</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.968738</td>\n",
       "      <td>1.091330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>1.743090</td>\n",
       "      <td>1.478815</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>-1.613058</td>\n",
       "      <td>-0.339364</td>\n",
       "      <td>0.613060</td>\n",
       "      <td>-0.921591</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.968738</td>\n",
       "      <td>0.131760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>-0.573694</td>\n",
       "      <td>0.901515</td>\n",
       "      <td>0.916013</td>\n",
       "      <td>-0.368904</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>1.361474</td>\n",
       "      <td>0.809503</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>-1.032270</td>\n",
       "      <td>1.412320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>-0.569844</td>\n",
       "      <td>1.743090</td>\n",
       "      <td>-0.624205</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>-0.081791</td>\n",
       "      <td>1.390762</td>\n",
       "      <td>-1.215717</td>\n",
       "      <td>0.809503</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.968738</td>\n",
       "      <td>0.844321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>1.754865</td>\n",
       "      <td>-0.573694</td>\n",
       "      <td>-0.284011</td>\n",
       "      <td>-1.091687</td>\n",
       "      <td>0.875251</td>\n",
       "      <td>-1.377440</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>-0.921591</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>-1.032270</td>\n",
       "      <td>0.324725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.569844  1.743090  0.169582 -1.091687 -0.464608  0.006661 -1.215717   \n",
       "1     1.754865 -0.573694 -2.304559  0.916013  0.301026 -1.377440 -0.006312   \n",
       "2    -0.569844 -0.573694 -1.191196 -1.091687 -0.943129 -1.031415  0.579935   \n",
       "3    -0.569844  1.743090  0.035566  0.916013  0.109617  0.006661  0.473128   \n",
       "4    -0.569844  1.743090  2.056114 -1.091687  1.736588  1.044737  0.810193   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7995  1.754865 -0.573694 -0.582970 -1.091687 -0.656016 -0.339364  0.703104   \n",
       "7996 -0.569844  1.743090  1.478815 -1.091687 -1.613058 -0.339364  0.613060   \n",
       "7997 -0.569844 -0.573694  0.901515  0.916013 -0.368904  0.006661  1.361474   \n",
       "7998 -0.569844  1.743090 -0.624205 -1.091687 -0.081791  1.390762 -1.215717   \n",
       "7999  1.754865 -0.573694 -0.284011 -1.091687  0.875251 -1.377440  0.511364   \n",
       "\n",
       "            7         8         9         10  \n",
       "0     0.809503  0.642595 -1.032270  1.106432  \n",
       "1    -0.921591  0.642595  0.968738 -0.748664  \n",
       "2    -0.921591  0.642595 -1.032270  1.485335  \n",
       "3    -0.921591  0.642595 -1.032270  1.276528  \n",
       "4     0.809503  0.642595  0.968738  0.558378  \n",
       "...        ...       ...       ...       ...  \n",
       "7995  0.809503  0.642595  0.968738  1.091330  \n",
       "7996 -0.921591  0.642595  0.968738  0.131760  \n",
       "7997  0.809503  0.642595 -1.032270  1.412320  \n",
       "7998  0.809503  0.642595  0.968738  0.844321  \n",
       "7999 -0.921591  0.642595 -1.032270  0.324725  \n",
       "\n",
       "[8000 rows x 11 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It shows the data with there row and column number.\n",
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.model import Sequential\n",
    "from keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the input layer and the first hidden layer.\n",
    "In the below line, we are going to add a further components of network to our model.\n",
    "In this add method take the input from dense class. Here, Dense create a fully connected two layer network which consist of 11 input neurons and 6 output layers. We are going to add more layers like this, so that six input neuron will be in the hidden layer of our complete network.\n",
    "The parameter for dense include :-\n",
    "1. Units - It represent the no. of last layer neurons for now.\n",
    "2. Kernel_initializer - The neural network need to start with some weights and then iteratively update them to better values. The term kernel_initiliser is a fancy term for which a statistical distribution or a function is used to initilise the weights in case of a statistical distribution. The library will generate numbers from the statistical distribution and use them as starting weights. The value of this parameter is uniform this means that all widths will be uniformly distributed in our network at first.\n",
    "3. Activation_function - We have use the Rectifier Activation Function(RELU).\n",
    "4. input_dim - It means dimention of input neuron, in our case it is 11. Whay it is 11? because we have 11 independent variables including 2 colums of geography, Thant's input_dim = 11. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input Layer\n",
    "classifier.add(Dense(units=6,kernel_initializer = 'uniform',activation = 'relu',input_dim = 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the next hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hidden Layer\n",
    "classifier.add(Dense(units=6,kernel_initializer = 'uniform',activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Output Layer\n",
    "In the output layer we need one neuron. Why we need only one neuron because we can see our dependent variable in binary form. therefore we have to predict in zero or one form. \n",
    "Below line is similar to previous one but here the unit parameter is 1 because we know that one neuron is enough to predict the output and the activation function is used here is sigmoid activation function because it provide the probability of the customer staying the bank or leaving the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Layer\n",
    "classifier.add(Dense(units = 1,kernel_initializer = 'uniform',activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have done with the creation of the structure of our first Artificial neural network.\n",
    "Now, In the next step we have to compile this network and also need to fit the data into the structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Artificial Neural Network   \n",
    "Compiler is the method of the tensor flow library, which the artificial neural network together.\n",
    "-The first parameter is the OPTIMISER its value in th code is adam which performs the stochastic gradient decent optimiser technique. \n",
    "-The second parameter is the Loss which represent the loss function. It is an optimization function which is used in case of training a classification model which classidy the data by predicting the probability wether the data is belong to one class or the other. One thing you need to aware of when you are doing a binary prediction similar to this one always use loss function 'binary_crssentropy' because the binary_crssentropy funtion computes the cross entropy loss between true levels and predicted levels.\n",
    "-The third parameter is METRICS it is about the valuation of our network to evaluate our artificial neural network model. In this we are going to use accuracy metrics which calculates how often predictions equal levels.\n",
    "This metric create two local variables tota and count that are used to compute the frequency with Y_pred matches Y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss  = 'binary_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the ANN model to the Training Set\n",
    "Next objective to fit our dataset into our model to do this, we are using the fit method that takes 4 arguments.\n",
    "First we have the value for the input neuron in this case it has X_train \n",
    "The second argument is the value for the output layer nerons.\n",
    "The third argument is the batch_size instead of comparing the predictions with real results one by one, it's better to perform this task in a batch so at a time it takes 10 rows.\n",
    "And the last argument is epochs it defines as the no. of time an algorithm visits the dataset. In other word epoch is one backward and one forward pass of the training examples.The neural network os train on certain no. of epochs to improve the accuracy over time. When you run the below line shows the accuracy in each epoch. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.4938 - accuracy: 0.7950\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4294 - accuracy: 0.7960\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.4244 - accuracy: 0.7960\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4205 - accuracy: 0.8076\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4176 - accuracy: 0.8236\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4156 - accuracy: 0.8301\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4144 - accuracy: 0.8295\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4126 - accuracy: 0.8317\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4118 - accuracy: 0.8316\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4110 - accuracy: 0.8330\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4099 - accuracy: 0.8336\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4093 - accuracy: 0.8334\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4081 - accuracy: 0.8341\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4079 - accuracy: 0.8341\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4069 - accuracy: 0.8340\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.4065 - accuracy: 0.8347\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.4061 - accuracy: 0.8349\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4057 - accuracy: 0.8344\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4057 - accuracy: 0.8355\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4050 - accuracy: 0.8356\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4048 - accuracy: 0.8338\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4043 - accuracy: 0.8351\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4039 - accuracy: 0.8344\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4039 - accuracy: 0.8346\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4037 - accuracy: 0.8367\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4035 - accuracy: 0.8364\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4032 - accuracy: 0.8354\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4030 - accuracy: 0.8351\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4027 - accuracy: 0.8359\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.8346\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4024 - accuracy: 0.8349\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4024 - accuracy: 0.8355\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4023 - accuracy: 0.8341\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4021 - accuracy: 0.8345\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4016 - accuracy: 0.8363\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4018 - accuracy: 0.8357\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4021 - accuracy: 0.8363\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4015 - accuracy: 0.8342\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4015 - accuracy: 0.8336\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4018 - accuracy: 0.8341\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4012 - accuracy: 0.8353\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4013 - accuracy: 0.8355\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4014 - accuracy: 0.8353\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4013 - accuracy: 0.8347\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4010 - accuracy: 0.8355\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4009 - accuracy: 0.8351\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4009 - accuracy: 0.8351\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4010 - accuracy: 0.8342\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8361\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4013 - accuracy: 0.8356\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8369\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4009 - accuracy: 0.8339\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4009 - accuracy: 0.8338\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4007 - accuracy: 0.8347\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4005 - accuracy: 0.8338\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8331\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8335\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4003 - accuracy: 0.8361\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4001 - accuracy: 0.8356\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8355\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4004 - accuracy: 0.8359\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4004 - accuracy: 0.8347\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4004 - accuracy: 0.8334\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4005 - accuracy: 0.8363\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4003 - accuracy: 0.8325\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4001 - accuracy: 0.8360\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4000 - accuracy: 0.8339\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4001 - accuracy: 0.8325\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4000 - accuracy: 0.8345\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.3998 - accuracy: 0.8360\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.3998 - accuracy: 0.8356\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4002 - accuracy: 0.8353\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4004 - accuracy: 0.8338\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4000 - accuracy: 0.8350\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3997 - accuracy: 0.8353\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4004 - accuracy: 0.8344\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4007 - accuracy: 0.8331\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4000 - accuracy: 0.8334\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3997 - accuracy: 0.8346\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3995 - accuracy: 0.8354\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3996 - accuracy: 0.8335\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4000 - accuracy: 0.8361\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3999 - accuracy: 0.8355\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4002 - accuracy: 0.8351\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3998 - accuracy: 0.8338\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3996 - accuracy: 0.8341\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3995 - accuracy: 0.8360\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4000 - accuracy: 0.8339\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 1s 998us/step - loss: 0.3996 - accuracy: 0.8365\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3997 - accuracy: 0.8356\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3998 - accuracy: 0.8338\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4000 - accuracy: 0.8341\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3993 - accuracy: 0.8353\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3998 - accuracy: 0.8364\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3992 - accuracy: 0.8347\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4002 - accuracy: 0.8357\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3999 - accuracy: 0.8341\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3998 - accuracy: 0.8360\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3996 - accuracy: 0.8357\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3998 - accuracy: 0.8345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29ee7c481c0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, Y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of Result\n",
    "In the below line we have done that if y_pred is greater than 0.5 then it become true else it become false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 952us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [False]\n",
      " [False]\n",
      " ...\n",
      " [False]\n",
      " [False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(Y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1537   58]\n",
      " [ 252  153]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.845"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(Y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd895b8607d04ec10fa0a756e6a7d005821edf313f80d8a1e1729fb63e9149e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

